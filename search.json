[
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "Quarto Blog",
    "section": "",
    "text": "Discrete Distributions\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nJan 2, 2027\n\n\nShreya Anbu\n\n\n\n\n\n\n\n\n\n\n\n\nDemonstration With Normal Distribution Example\n\n\n\n\n\n\nStatistics\n\n\n\n\n\n\n\n\n\nFeb 25, 2025\n\n\nShreya Anbu\n\n\n\n\n\n\n\n\n\n\n\n\nIntroduction to SciPy\n\n\n\n\n\nIntroduction Post\n\n\n\n\n\nFeb 25, 2025\n\n\nShreya Anbu\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/welcome.html",
    "href": "posts/welcome.html",
    "title": "Introduction to SciPy",
    "section": "",
    "text": "Introduction to SciPy\n\n\nscipy.stats Module\nIt is a very helpful module used for statistical analysis and modeling.\n\nThis module contains a large number of probability distributions, summary and frequency statistics, correlation functions and statistical tests, masked statistics, kernel density estimation, quasi-Monte Carlo functionality, and more.\n\nYou can find more information on the official documentation page.\n\n\nInstallation\nIf you already have Python installed, you can install SciPy with pip by executing the following in a terminal/shell:\npython -m pip install scipy\nWhile using Jupyter Notebook or Google Colab, just use the following line before writing the main code:\nimport scipy\n\n\nComparison with PyTorch Distributions\nCan‚Äôt both of these modules used for modeling distributions? Why do they exist as two separate models then?  PyTorch module provides more functionality for deep learning applications, whereas SciPy is more suitable for statistical modeling, having a more user-friendly interface for statisticians.\n\n\n\n\n\n\n\n\nFeature\nPyTorch Version\nScipPy Version\n\n\n\n\nCreating a Distribution\ntorch.distributions.Poisson(rate_param)\nscipy.stats.poisson(rate_param)\n\n\nCreating a Sample\ndist.sample((n, ))\ndist.rvs(size = n)\n\n\nCalculating PMF\ndist.log_prob(x).exp()\ndist.pmf(x)\n\n\n\nWe can see with the last comparison how SciPy has more direct, easy to read functions for obtaining basic values out of a distribution.\n\n\nDifferent Kinds of Distributions Available\nThere are a lot of things contained by the scipy.stats module, but we will be looking into the different kinds of distributions available for us to use. There are three kinds: continuous, discrete and multivariate. \n\nContinuous Distribution: Random variable can take on any value within a specified range, meaning there are infinitely many possible values between any two points;\nDiscrete Distribution: Random variable can only take on a finite number of distinct values, usually whole numbers;\nMultivariate Distribution: Describes the probability of multiple random variables occurring simultaneously, considering their potential relationships and correlations with each other.\n\nThere are tons of distributions available under each kind that we can use, which can be explored again in the official documentation.\n\n\n\nScreenshot from official documentation\n\n\nThe above image shows the start of the list of functionalities, which barely scratches the surface."
  },
  {
    "objectID": "posts/Normal distribution.html",
    "href": "posts/Normal distribution.html",
    "title": "Demonstration With Normal Distribution Example",
    "section": "",
    "text": "import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport scipy.stats as stats\n\nDATA:\nthe data is taken from the website - https://www.kaggle.com/datasets/mustafaali96/weight-height?resource=download\n\ndf=pd.read_csv(\"weight-height.csv\")\nprint(df.head())\n\n  Gender     Height      Weight\n0   Male  73.847017  241.893563\n1   Male  68.781904  162.310473\n2   Male  74.110105  212.740856\n3   Male  71.730978  220.042470\n4   Male  69.881796  206.349801\n\n\nDATA VISUALIZATION:\nPlotting a histogram depicting the number of people having a particular height.\n\nplt.hist(df[\"Height\"], bins =60)\nplt.xlabel(\"Height\")\nplt.ylabel(\"Density\")\nplt.title(\"Histogram of Height of people\")\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\nFITTING DISTRIBUTION\n\nThe function estimates the parameters of a normal distribution that best fit the ‚ÄúHeight‚Äù data.\nIt finds the mean (Œº) and standard deviation (œÉ) that maximize the likelihood of observing the dataset under a normal distribution.\n\n\nloc, scale = stats.norm.fit(df[\"Height\"])\nloc, scale\n\n(66.36755975482124, 3.84733573955754)\n\n\n\nloc ‚Üí Mean (Œº) of the normal distribution.\nscale ‚Üí Standard deviation (œÉ) of the normal distribution.\n\n\nx_vals = np.linspace(min(df[\"Height\"]), max(df[\"Height\"]), 1000)\n\nPlotting PDF(Probability Density Function) for the Height dataset\nThe Probability Density Function (PDF) of a normal distribution describes the likelihood of a continuous random variable taking on a specific value. It is mathematically defined as:  where, - f(x) ‚Üí Probability density at point ùë•. - ùúá ‚Üí Mean (expected value) of the distribution (center of the bell curve). - ùúé ‚Üí Standard deviation (spread of the distribution). - ùúé^2 ‚Üí Variance.\n\npdf=stats.norm.pdf(x_vals, loc=loc, scale=scale)\nplt.plot(x_vals, pdf, color=\"black\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Density\")\nplt.title(\"PDF curve of Height of people\")\nplt.show()\n\n\n\n\n\n\n\n\nComparing histogram with the pdf curve\n\n# Fit a normal distribution\nmu, sigma = stats.norm.fit(df[\"Height\"])  # Should be close to (0,1) for standardized data\n\n# Generate PDF values\nx_vals = np.linspace(min(df[\"Height\"]), max(df[\"Height\"]), 1000)\npdf_vals = stats.norm.pdf(x_vals, mu, sigma)\n\n# Plot the fitted normal distribution\nplt.figure(figsize=(8, 5))\nplt.plot(x_vals, pdf_vals, label=f\"Normal Fit (Œº={mu:.2f}, œÉ={sigma:.2f})\", color='red')\nplt.hist(df[\"Height\"], bins=20, density=True, alpha=0.5, label=\"Histogram\")\nplt.xlabel(\"Standardized Return\")\nplt.ylabel(\"Density\")\nplt.title(\"Fitted Normal Distribution PDF\")\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nInterpretation: - If the histogram closely follows the fitted red curve, it means the data is well-approximated by a normal distribution. - If the histogram deviates significantly, the normal distribution might not be a good fit, and you might need to try other distributions (e.g., stats.expon.fit() for exponential, stats.lognorm.fit() for log-normal).\nWhy we cannot plot a PMF of this dataset? - PMF V/S PDF 1. PMF is for Discrete Data The PMF (Probability Mass Function) applies to discrete random variables, meaning variables that take on countable values (e.g., number of students, number of cars). It gives the probability of exact values occurring.\n\nPDF is for Continuous Data Our dataset consists of continuous stock returns (e.g., real numbers like 0.52, -1.23, 1.89). For continuous distributions, the probability of a single exact value occurring is zero because there are infinitely many possible values. Instead of a PMF, we use a Probability Density Function (PDF) to describe the relative likelihood of values falling within an interval.\n\nSince stock returns are continuous, we cannot use a PMF. Instead, we must use a PDF (such as a normal distribution or kernel density estimation) to describe their distribution.\nPlotting CDF(Cummulative Distribution Function) for the height dataset\n\nThe Cumulative Distribution Function (CDF) gives the probability that a random variable ùëã is less than or equal to a specific value ùë•, i.e., F(x)=P(X‚â§x)\nIt helps in understanding the cumulative probability of the data.\n\n\ncdf=stats.norm.cdf(x_vals, loc=loc, scale=scale)\nplt.plot(x_vals, cdf, color=\"black\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Cummulative Probability\")\nplt.title(\"CDF curve of Height of people\")\nplt.show()\n\n\n\n\n\n\n\n\nPlotting PPF(Percent Point Function) for the height dataset\nPPF The Percent-Point Function (PPF) is the inverse of the Cumulative Distribution Function (CDF). It is also called the quantile function because it helps find the value of a random variable at a given probability.\nIn simple terms, if:\nùêπ(ùë•)=ùëÉ(ùëã‚â§ùë•)\nthen PPF finds ùë• for a given probability ùëù, i.e., ùë•=ùêπ‚àí1(ùëù)\nThis helps in determining critical values for a normal distribution.\n\np_vals = np.linspace(0.01, 0.99, 1000)\nppf=stats.norm.ppf(p_vals, loc=loc, scale=scale)\nplt.plot(p_vals, ppf, color=\"black\")\nplt.ylabel(\"Height\")\nplt.xlabel(\"Cummulative Probability\")\nplt.title(\"PPF curve of Height of people\")\nplt.show()\n\n\n\n\n\n\n\n\nUse of CDF\nFinding the probability that the variable X takes values lower than 70 and then representing it graphically\n\ncdf_1=stats.norm.cdf(70, loc=loc, scale=scale)\ncdf_1\n\n0.827452052034947\n\n\n\ncdf=stats.norm.cdf(x_vals, loc=loc, scale=scale)\nplt.plot(x_vals, cdf, color=\"black\")\nplt.vlines(70, 0, cdf_1, linestyle=\":\")\nplt.hlines(cdf_1, 50, 70, linestyle=\":\")\nplt.xlabel(\"Height\")\nplt.ylabel(\"Cummulative Probability\")\nplt.title(\"CDF curve of Height of people\")\nplt.show()\n\n\n\n\n\n\n\n\nReal world usage of Normal distribution\nThe normal distribution (Gaussian distribution) appears in many real-world scenarios because of the Central Limit Theorem (CLT), which states that the sum of many independent random variables tends to follow a normal distribution. Here are some real-world applications:\n\nHuman Characteristics (Height, Weight, IQ, etc.)\nTest Scores (SAT, IQ, GRE, etc.)\nMeasurement Errors in Experiments\nFinancial Markets (Stock Prices & Returns)\nBlood Pressure & Other Medical Metrics\nManufacturing & Quality Control\nSignal Processing & Communication Systems\nSports Performance & Player Statistics\nPhysics & Natural Phenomena\n\nHence, the normal distribution is essential for understanding data patterns, making predictions, and conducting statistical analyses. Its predictability and mathematical properties make it a cornerstone in probability theory, science, and engineering."
  },
  {
    "objectID": "posts/Static_Version_of_Discrete_Distributions_in_SciPy (1).html",
    "href": "posts/Static_Version_of_Discrete_Distributions_in_SciPy (1).html",
    "title": "Discrete Distributions",
    "section": "",
    "text": "Bernoulli distribution\nBernoulli Distribution is a type of discrete probability distribution where every experiment conducted asks a question that can be answered only in yes or no. In other words, the random variable can be 1 with a probability p or it can be 0 with a probability (1 - p). Such an experiment is called a Bernoulli trial. A pass or fail exam can be modeled by a Bernoulli Distribution.\nSuppose there is an experiment where you flip a coin that is fair. If the outcome of the flip is heads then you will win. This means that the probability of getting heads is p = 1/2. If X is the random variable following a Bernoulli Distribution, we get P(X = 1) = p = 1/2.\nA binomial random variable, X, is also known as an indicator variable. This is because if an event results in success then X = 1 and if the outcome is a failure then X = 0. X can be written as X ‚àºBernoulli (p), where p is the parameter. The formulas for Bernoulli distribution are given by the probability mass function (pmf) and the cumulative distribution function (CDF).\n\n\n\nimage.png\n\n\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom scipy.stats import bernoulli\nimport ipywidgets as widgets\nfrom ipywidgets import interact\n\np = 0.3\nplt.figure(figsize=(5, 4))\nx = np.array([0, 1])\npmf_vals = bernoulli.pmf(x, p)\nplt.plot(x, pmf_vals, 'bo', ms=8, label='Bernoulli PMF')\nplt.vlines(x, 0, pmf_vals, colors='b', lw=5, alpha=0.5)\nplt.xticks([0, 1])\nplt.xlabel(\"X\")\nplt.ylabel(\"PMF\")\nplt.title(f\"Bernoulli PMF (p={p:.2f})\")\nplt.legend()\nplt.grid()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nBinomial Distribution\nA discrete probability distribution that includes the number of trials n, probability of success and probability of failure is called as Binomial distribution.\nThe probability mass function of the Binomial distribution is given by:\n\n\n\nimage.png\n\n\nThe probability mass function above is defined in the ‚Äústandardized‚Äù form. To shift distribution use the loc parameter. Specifically, binom.pmf(k, n, p, loc) is identically equivalent to binom.pmf(k - loc, n, p).\nBinomial Distribution is used where we have only two possible outcomes. Let‚Äôs see some of the areas where Binomial Distribution can be used:\n\nTo find the number of male and female students in an institute.\nTo find the likeability of something in Yes or No.\nTo find defective or good products manufactured in a factory.\nTo find positive and negative reviews on a product.\nVotes are collected in the form of 0 or 1.\n\n\nimport scipy.stats as stats\nfrom scipy.stats import binom\n\n\nn = 10\np = 0.4\nx = np.arange(0, n + 1)\nplt.figure(figsize=(7, 5))\nplt.plot(x, stats.binom.pmf(x, n, p), 'bo', ms=8, label='binom pmf')\nplt.vlines(x, 0, stats.binom.pmf(x, n, p), colors='b', lw=5, alpha=0.5)\nplt.title(f'Binomial Distribution PMF (n={n}, p={p:.2f})')\nplt.xlabel('Number of Successes')\nplt.ylabel('Probability Mass')\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\n\n\nPoisson Distribution\nA Poisson distribution is commonly seen in real life situations where events occur independently over a fixed time interval, such as: the number of phone calls received at a call center per hour, the number of customers arriving at a store in a given time frame, the number of defects in a manufactured product batch, or the number of car accidents at a particular intersection per day.\nWhenever there is an average rate of an event happening, it can most probably be modelled by a poisson distribution. It will help us predict the probability of the same event happening at different frequencies.\n\nfrom scipy.stats import poisson\n\nmu = 1.2\nx = np.arange(poisson.ppf(0.01, mu), poisson.ppf(0.99, mu))\nplt.plot(x, poisson.pmf(x, mu), 'bo', ms=8, label='Poisson PMF')\nplt.vlines(x, 0, poisson.pmf(x, mu), colors='b', lw=5, alpha=0.5)\nplt.title(f'Poisson Distribution (mu={mu})')\nplt.xlabel('x')\nplt.ylabel('PMF')\nplt.show()"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ES 114 Assignment",
    "section": "",
    "text": "This blog website has been created as part of the course of our BTech first year in IIT Gandhinagar, titled ‚ÄòProbability, Statistics and Data Visualizations‚Äô.\nStudent‚Äôs Details: Prachi Jindal (24110264) Shah Khushi (24110326) Shreya Anbu (24110331)"
  }
]